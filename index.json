[{"authors":["jaredfern"],"categories":null,"content":"I am a doctoral student in the Language Technologies Institute at Carnegie Mellon University where I am advised by Yonatan Bisk and Emma Strubell.\nMy research interests are in characterizing and evaluating the impact of machine learning workloads, with the goal of designing computationally and energy efficient systems for artificial intelligence. My research is generously supported by the NSF Graduate Research Fellowship.\nPreviously, I received my B.S. in Computer Science and B.S. in Electrical Engineering at Northwestern University where I worked with Doug Downey and Thrasos Pappas. I\u0026rsquo;ve also spent time at Google, Meta FAIR, and the Allen Institute for Artificial Intelligence.\nEmail: jaredfern [at] cmu.edu\nOut-of-Date News: $\\quad$ [April 2025] $\\quad$ Preliminary work won a Best Proposal Award at the CCAI Workshop at ICLR! $\\quad$ [April 2025] $\\quad$ Presented joint work on low-power distributed training at SIGBOVIK! $\\quad$ [Nov. 2024] $\\quad$ Presented our paper on continual pretraining for LLMs at EMNLP! $\\quad$ [June 2024] $\\quad$ Interning at Meta FAIR studying the efficiency of distributed training! $\\quad$ [Dec. 2023] $\\quad$ Presented work on deep learning framework efficiency at EMNLP!\n","date":1744329600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1744329600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://localhost:1313/author/jared-fernandez/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/author/jared-fernandez/","section":"authors","summary":"I am a doctoral student in the Language Technologies Institute at Carnegie Mellon University where I am advised by Yonatan Bisk and Emma Strubell.\nMy research interests are in characterizing and evaluating the impact of machine learning workloads, with the goal of designing computationally and energy efficient systems for artificial intelligence. My research is generously supported by the NSF Graduate Research Fellowship.\n","tags":null,"title":"Jared Fernandez","type":"authors"},{"authors":["jaredfern"],"categories":null,"content":"I am a doctoral student in the Language Technologies Institute at Carnegie Mellon University where I am advised by Yonatan Bisk and Emma Strubell.\nMy research interests are in the computational efficiency of machine learning models, with a focus on applications in natural language processing and computer vision. My research is generously supported by the NSF Graduate Research Fellowship.\nPreviously, I received my B.S. in Computer Science and B.S. in Electrical Engineering at Northwestern University where I worked with Doug Downey and Thrasos Pappas. I\u0026rsquo;ve also spent time as a Software Engineer at Google.\nEmail: jaredfern [at] cmu.edu\nNews: $\\qquad$ [April 2021] $\\quad$ Awarded an NSF Graduate Research Fellowship! $\\qquad$ [Aug. 2020] $\\quad$ Starting as a PhD student at Carnegie Mellon University! $\\qquad$ [Nov. 2019] $\\quad$ Starting fulltime as a Software Engineer at Google!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4036ab9376e924db20b9e42fb1307811","permalink":"http://localhost:1313/author/jared-fernandez/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jared-fernandez/","section":"authors","summary":"I am a doctoral student in the Language Technologies Institute at Carnegie Mellon University where I am advised by Yonatan Bisk and Emma Strubell.\nMy research interests are in the computational efficiency of machine learning models, with a focus on applications in natural language processing and computer vision. My research is generously supported by the NSF Graduate Research Fellowship.\n","tags":null,"title":"Jared Fernandez","type":"authors"},{"authors":["Jared Fernandez","Clara Na","Vashsisth Tiwari","Yonatan Bisk","Sasha Luccioni","Emma Strubell"],"categories":null,"content":"","date":1744329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1744329600,"objectID":"6be31cba0926735b2da9dc978ca621e9","permalink":"http://localhost:1313/publication/inference_energy/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/inference_energy/","section":"publication","summary":"The 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025.","tags":["energy","efficiency","inference"],"title":"Energy Considerations of Large Language Model Inference and Efficiency Optimizations","type":"publication"},{"authors":["Jared Fernandez","Luca Wehrstedt","Leonid Shamis","Mostafa Elhoushi","Kalyan Saladi","Yonatan Bisk","Emma Strubell","Jacob Kahn"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"1a28f0bbabbbc4a481ed5bb86a9a3d49","permalink":"http://localhost:1313/publication/hardware-scaling/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/hardware-scaling/","section":"publication","summary":"Transactions on Machine Learning Research, 2025.","tags":["efficiency","energy","training"],"title":"Efficient Hardware Scaling and Diminishing Returns in Large-Scale Training of Language Models","type":"publication"},{"authors":["Jacob Morrison","Clara Na","Jared Fernandez","Tim Dettmers","Emma Strubell","Jesse Dodge"],"categories":null,"content":"","date":1740787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740787200,"objectID":"ee3f8b918e53be5c1e9e0ea4cdf5610b","permalink":"http://localhost:1313/publication/olmo-impact/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/olmo-impact/","section":"publication","summary":"The Thirtheenth International Conference on Learning Representations (ICLR), 2025.","tags":["efficiency"],"title":"Holistically Evaluating the Environmental Impact of Creating Language Models","type":"publication"},{"authors":["Jared Fernandez","Yonatan Bisk","Emma Strubell"],"categories":null,"content":"","date":1730419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730419200,"objectID":"28c20bd5a8f0bd075392545b0ce6b47f","permalink":"http://localhost:1313/publication/gradient_trace/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/gradient_trace/","section":"publication","summary":"Findings of the Association for Computational Linguistics: EMNLP (EMNLP Findings), 2024.","tags":["continual-learning"],"title":"Gradient Localization Improves Lifelong Pretraining of Language Models","type":"publication"},{"authors":["Jared Fernandez","Saujas Vaduguru","Sanket Vaibhav Mehta","Yonatan Bisk","Emma Strubell"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"bebc06c96a7ecc56a4103b83f9074965","permalink":"http://localhost:1313/publication/csaw/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/csaw/","section":"publication","summary":"Workshop on High Dimensional Learning Dynamics at ICML, 2023.","tags":["distribution shift","weight averaging"],"title":"Adapting to Gradual Distribution Shifts with Continual Weight Averaging","type":"publication"},{"authors":["Jared Fernandez","Jacob Kahn","Clara Na","Yonatan Bisk","Emma Strubell"],"categories":null,"content":"","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"57317db05f6122816c9b0cdfa87ffb91","permalink":"http://localhost:1313/publication/framework/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/framework/","section":"publication","summary":"Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023.","tags":["efficiency"],"title":"The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment","type":"publication"},{"authors":["Xiaopeng Lu","Lynnette Ng","Jared Fernandez","Hao Zhu"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"e9814c028bf15952f801dea969baad19","permalink":"http://localhost:1313/publication/cigli/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/cigli/","section":"publication","summary":"Fourth Workshop on Closing the Loop Between Vision \u0026 Language, 2021.","tags":["multimodal","reasoning","vision and language"],"title":"CIGLI: Conditional Image Generation from Language and image","type":"publication"},{"authors":["Yiben Yang","Chaitanya Malaviya","Jared Fernandez","Swabha Swayamdipta","Ronan Le Bras","Ji-Ping Wang","Chandra Bhagavatula","Yejin Choi","Doug Downey"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"2a0a277561a034009837d0ce1e602cdb","permalink":"http://localhost:1313/publication/gdaug/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/gdaug/","section":"publication","summary":"Findings of the Association for Computational Linguistics: EMNLP, 2020.","tags":["data augmentation","commonsense","question answering"],"title":"Generative Data Augmentation for Commonsense Reasoning","type":"publication"},{"authors":["Michael Chen","Mike D'arcy","Alisa Liu","Jared Fernandez","Doug Downey"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"b9966e05ba0d9a68862f41435e4eef65","permalink":"http://localhost:1313/publication/codah/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/codah/","section":"publication","summary":"Workshop on Evaluating Vector Space Representations for NLP (RepEval), 2019.","tags":["common sense","dataset","question-answering"],"title":"CODAH: An Adversarially Authored Question-Answer Dataset for Common Sense","type":"publication"},{"authors":["Jared Fernandez","Doug Downey"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"451c5aa4af41e87d0cf02d2459917729","permalink":"http://localhost:1313/publication/rnn-is/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/rnn-is/","section":"publication","summary":"ACL Student Research Workshop (ACL-SRW), 2018.","tags":["efficiency","rnnlm","language modeling"],"title":"Sampling Informative Training Data for RNN Language Models","type":"publication"},{"authors":["Jared Fernandez","Zhaocheng Yu","Doug Downey"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"2ae46e9c03567e136c54342bb01ca548","permalink":"http://localhost:1313/publication/vecshare/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/vecshare/","section":"publication","summary":"Empirical Methods in Natural Language Processing (EMNLP), 2017.","tags":["embedddings"],"title":"VecShare: A Framework for Sharing Word Representation Vectors","type":"publication"}]